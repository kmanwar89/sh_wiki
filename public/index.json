[
{
	"uri": "http://localhost/contribution/get-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "This guide will show you how to setup a local environment for you to edit, create, or update content!\nPlease note, if you wish to make a simple edit, you can always submit a quick pull request by utilizing the edit button on the file in question directly on the GitHub repo online.\nInstalling Hugo Since we are using Hugo, getting your local site up and running is fairly simple.\nOS independent Since Hugo is cross-platform, and OS choice is far from uniform in this community, I won\u0026rsquo;t go into how to get Hugo functioning on your OS of choice. Follow the Getting Started. It\u0026rsquo;s not difficult.\nOnce you can successfully get a version number from the command hugo version, you\u0026rsquo;re ready to continue here.\nFork and clone operations First thing you\u0026rsquo;ll want to do is to Fork the GitHub Repository for the /r/selfhosted wiki. You\u0026rsquo;ll work within a repo that syncs with your own accounts\u0026rsquo; fork of the repository.\nSince we use a theme that has its own GitHub Repository, there is an extra flag we must add to our git clone command.\nFirst, clone the forked repo into your local machine. The \u0026ldquo;recurse-submodules\u0026rdquo; flag should allow you to automatically pull in the git repo for the theme, as well.\ngit clone --recurse-submodules https://github.com/{YOUR_USER_NAME}/wiki.git \u0026ndash; Be sure to modify this url to match your actual username and git repository name.\nMove to the directory that has just been cloned and make sure the themes/hugo-theme-learn/ folder has content.\ncd wiki \u0026amp;\u0026amp; ls themes/hugo-theme-learn\nOnce confirmed, copy the example config.toml file. Unless you\u0026rsquo;re doing some abstract hosting environment for your local development machine, this should work as-is.\ncp config.toml.example config.toml\nRun the server locally with the Hugo\u0026rsquo;s server command\nhugo serve\nYou should see some output about the success/launch of the local server, similar to below:\n$ hugo serve Start building sites … | EN -------------------+----- Pages | 22 Paginator pages | 0 Non-page files | 0 Static files | 75 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Built in 84 ms Watching for changes in /home/kmisterk/wiki/{archetypes,content,data,layouts,static,themes} Watching for config changes in /home/kmisterk/wiki/config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop Navigate your browser to http://localhost:1313 and you should see the site live on your local machine.\nYou\u0026rsquo;re now ready to start adding and/or editing content.\n"
},
{
	"uri": "http://localhost/guides/devops-toolchains/docker/cheatsheet/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Information gathering docker ps -a or docker container list - List all containers docker ps -aq - List all container ID\u0026rsquo;s\nThis command is useful because it can be called as a variable [ex: $(docker ps -aq)] and fed into other commands (such as docker stop or docker rm) as shown in the command above. docker image list - List all docker images docker system df - Docker disk usage docker system info - System info\nInteracting with containers Container creation docker start - starts any stopped container docker run - creates \u0026amp; starts the newly-created container\nStopping containers docker stop $(docker ps -aq) - stop all running containers by feeding in container ID\u0026rsquo;s docker stop \u0026lt;container name | container ID\u0026gt; - stop a single container using SIGTERM\nAlternate syntax: docker container stop \u0026lt;container name | container ID\u0026gt; docker kill \u0026lt;container name | container ID\u0026gt; - kill a single container using SIGKILL\nBuilding Note: the file must be called Dockerfile, verbatim. It is not a file extension, but that is the default filename that the docker build command looks for.\ndocker build . - build an image from a Dockerfile in the current directory docker build [-t username/repository:tag] . - build an image from a Dockerfile in the current directory, and optionally tag it. docker build -t [username/repository:tag] - \u0026lt; [name of dockerfile] - build an image using a Dockerfile read from STDIN docker build -f [Dockerfile] . - builds from a Dockerfile in an alternate location\ndocker run --name \u0026lt;human readable name\u0026gt; \u0026lt;image name\u0026gt; - create a container from an image, and give it a custom name (instead of the system generated ones)\nWorking with Docker Compose Docker Compose is a way of building and interacting with containers using a YAML-formatted file. This has the unique advantage of not having to type out the commands each time, but also allowing for manipulation of networks, databases and inter-connections between containers in a very simple and seamless way.\ndocker compose up -d - create and start containers from a docker-compose.yml file in a daemonized or \u0026ldquo;detached\u0026rdquo; fashion, i.e. the container will continue running in the background docker-compose up --force-recreate --build -d - quick one-liner to pull down changes to a container after modifying the compose file. Can optionally run docker compose up -d again and changes will be reflected\nManaging Images docker image rm \u0026lt;image name\u0026gt; - deletes an image from local image store docker rmi $(docker images -q) - removes all unused images docker tag \u0026lt;imagename:oldtag\u0026gt; \u0026lt;username/repository:newtag\u0026gt; - renames/re-tags an image\nUsing Repositories/Docker Hub docker pull \u0026lt;image name\u0026gt;:[tag] - retrieve image from repository docker push \u0026lt;username/repository:tag\u0026gt; - pushes image to a repository.\nCurrenly, Docker Hub limits to a single private repository in the format \u0026lt;username/repo name\u0026gt;. Tags can be used to manage multiple images within the repository. Container and System Management Deleting images \u0026amp; containers docker rm $(docker ps -aq) - remove all containers based on container ID. Will not remove containers that haven\u0026rsquo;t been stopped\nGeneral system cleanup docker system prune [--volumes] - removes all stopped containers, networks, dangling images/cache and optionally any unused volumes docker volume prune - removes any unused volumes\nUpgrading an image This is useful when a new version is released; rather than deleting the image and re-downloading it, use the following condensed command to upgrade an image. Source docker-compose up --force-recreate --build -d Can also use: docker-compose pull\nNaming and interacting with containers When working in a docker-compose file, there are two ways to refer to a container - the name that is present while defining the various components of the compose file, or the \u0026ldquo;container_name\u0026rdquo; directive. For example:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a5130a954a0f jc21/nginx-proxy-manager \u0026#34;/init\u0026#34; 3 seconds ago Up 1 second 0.0.0.0:80-81-\u0026gt;80-81/tcp, :::80-81-\u0026gt;80-81/tcp, 0.0.0.0:443-\u0026gt;443/tcp, :::443-\u0026gt;443/tcp npm_prod The name \u0026ldquo;npm_prod\u0026rdquo; is shown above in the far-right column under \u0026ldquo;Names\u0026rdquo;, and this was defined in the compose file under the \u0026ldquo;container_name\u0026rdquo; option:\nversion: \u0026#39;3\u0026#39; services: app: image: jc21/nginx-proxy-manager container_name: npm_prod \u0026lt;-------- If this option is removed, the container will inherit a default name in the form of \u0026lt;image name_service name_numeral\u0026gt;. For example:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c00ecf065e9d jc21/nginx-proxy-manager \u0026#34;/init\u0026#34; 1 second ago Up 1 second 0.0.0.0:80-81-\u0026gt;80-81/tcp, :::80-81-\u0026gt;80-81/tcp, 0.0.0.0:443-\u0026gt;443/tcp, :::443-\u0026gt;443/tcp npm_app_1 The docker-compose file now looks like:\nversion: \u0026#39;3\u0026#39; services: app: image: jc21/nginx-proxy-manager This is a helpful option to help keep track of various containers, to help separate dev/prod or for any other reson where a unique identifier would be helpful, or necessary\nInteracting with a container Log management docker container logs --tail 100 \u0026lt;container name\u0026gt; - delete last 100 lines of a containers\u0026rsquo; log docker logs \u0026lt;container name\u0026gt; --follow - equivalent of tail -f functionality to view logs in real time. Useful for diagnosis\nGet a console into the container docker exec -it \u0026lt;container name\u0026gt; /bin/bash\nVolumes There are two main types of volumes - bind and mount, explained by the following graphic:\nThe syntax looks like the following \u0026ndash;\nFor docker-compose: services: service_name: image: image_repo/image_name:tag container_name: name_of_container volumes: - /path/on/host/machine/to/expose:/path/within/the/docker/container:\u0026lt;optional permissions\u0026gt; For docker run: docker run -detached -port_to_expose ### \\ -v /path/on/host/machine/to/expose:/path/within/the/docker/container:\u0026lt;optional permissions\u0026gt; Docker Networking Create docker network docker network create \u0026lt;name\u0026gt;\nAttach a container to that network docker run \u0026ndash;network \u0026lt;name\u0026gt; \u0026ndash;name=name of container [image name]\nExposing Host and Container ports Depending on the service running, the container will require either a private, ephemeral port that is decided by the developers, or an exposed, well-known port. For a secure setup behind a reverse proxy (such as NPM), only 3 ports should be bound to the host machine (and thereby forwarded by the router/exposed to the Internet): 80, 81 and 443.\nOther services, such as Pi-Hole, AdGuard or other services that operate on well-known port numbers may require those ports to be exposed (such as 53 for DNS). When creating a docker-compose file, ports are exposed with the following syntax:\nports: - \u0026lt;host port\u0026gt;:\u0026lt;container port\u0026gt; For example, NPM requires 80, 81 and 443, so the port mapping should look like:\nports: - 80:80 - 81:81 - 443:443 On the router, these ports would be directly forwarded. However, since only 1 service can use a port at a time, and multiple services may require well-known ports such as 80 or 443, custom ports can be bound to those, and Docker\u0026rsquo;s service handles the \u0026ldquo;routing\u0026rdquo; of the traffic into the container. If a service requires port 80, for instance, this limitation can be overcome as follows:\nports: - 8080:80 "
},
{
	"uri": "http://localhost/",
	"title": "/r/Selfhosted Official Wiki",
	"tags": [],
	"description": "",
	"content": "/r/SelfHosted Official Wiki Welcome to the Wiki We welcome you to explore the pages here and familiarize yourself with the layout of the wiki. If you have any suggestions for better/alternative organization methods, or feedback/questions in general about the wiki, feel free to reach out to:\nour subreddit moderators Discord channel Matrix | View online: Elements Getting started Here you can find a couple of articles about the best ways to get started based on what you\u0026rsquo;re looking to accomplish.\nLearn about what self-hosting is Learn about hosting websites Learn about using reverse proxies Learn about self-hosted alternatives to popular services and providers Learn about the most common ways to self-host Some specific guides and tips/tricks for popular self-hosted applications to help avoid pitfalls/wasted time "
},
{
	"uri": "http://localhost/contribution/adding-a-page/",
	"title": "Adding a Page",
	"tags": [],
	"description": "",
	"content": "Adding pages with Hugo is easy! Each page has its own folder with an _index.md file. Make a new folder with an _index.md file inside.\nThe _index.md file looks like this\n--- Title: TITLE HERE Other metafields: metafield content --- Content! See? It\u0026rsquo;s easy!\nPage content uses Markdown syntax for text styles, formatting, hyperlinks, and all kinds of stuff.\nYou can find more documentation about Markdown support in Hugo here: https://www.markdownguide.org/tools/hugo/\nIf you want more information on how to use Markdown syntax in content, please visit: https://daringfireball.net/projects/markdown/basics\n"
},
{
	"uri": "http://localhost/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/contribution/",
	"title": "Contribution Guidelines",
	"tags": [],
	"description": "",
	"content": "This guide aims to showcase how to contribute to this wiki, including best practices, pull requests, adding and editing articles and content, and file structure.\n"
},
{
	"uri": "http://localhost/guides/devops-toolchains/",
	"title": "DevOps Toolchains",
	"tags": [],
	"description": "",
	"content": "DevOps is the next evolution of agile methodologies. A cultural shift that brings development and operations teams together.\nNo specific DevOps tool or tools equate to “doing DevOps”, but there are plenty of tools that help enable the best practices that a DevOps culture promotes. Generally, these are tools that help streamline DevOps pipelines.\nDevOps Toolchains Gitlab Installation on Kuberntes Docker "
},
{
	"uri": "http://localhost/getting-started/difficulty-tiers/",
	"title": "Difficulty Tiers",
	"tags": ["easy", "normal", "hard"],
	"description": "",
	"content": "How hard is stuff? Stuff can be hard. Stuff can also be easy. Some things here are hard. Some things are easy. Let\u0026rsquo;s define some difficulty tiers.\nEasy / Basic No worries here, folks! Everything should be done with a GUI with maybe some stuff needing a command or two, or maybe just editing some text files. Instructions should come in picture form.\nOperating systems in this tier are Ubuntu Linux, Windows, or OS X.\nNormal / Intermediate Still not hard, but you\u0026rsquo;re expected to know how not to break things in a terminal or text-based interface. GUI is optional, but used on occasion.\nOperating systems in this tier are Debian Linux, Red Hat Enterprise Linux, Rocky Linux.\nHard / Advanced You are at home with a terminal and all things can be accomplished with it. You know how not to break a system with a terminal, but also how to fix serious system-level issues.\nOperating systems in this tier are Arch, *BSD, Slackware Linux, Gentoo Linux.\nGuru / Expert You compile your software from source and surf the web with Lynx. If you don\u0026rsquo;t know what that means you are a poser.\nYou run Suicide Linux because anything less is for little babies.\n"
},
{
	"uri": "http://localhost/guides/devops-toolchains/docker/introduction/",
	"title": "Docker Primer",
	"tags": ["normal"],
	"description": "",
	"content": " Introduction This guide will provide a cursory, high-level overview of Docker. This is not intended to replace the official Docker documentation, but is instead meant to distill the contents into an easy-to-read format, specifically aimed at self hosters.\nDocker is available on all Windows, Mac and Linux, but is used most often on Linux machines. Both Mac and Windows have a graphical interface called Docker Desktop which can be used to interface with containers. Recently, DD4L (Docker Desktop for Linux) was made public, unifying the experience across all operating systems. Please note that DD4L runs a VM in the background, so it may be more resource-heavy, but is good for learning the basics.\nIn most deployments, especially in the Enterprise space, Docker (and similar containerization solutions) will be run on Linux servers, so it\u0026rsquo;s useful to get some practice with the command line. This guide will provide common command-line commands for managing Docker containers, volumes, images and networks.\nFor those who are more comfortable with a graphical experience, great solutions such as Portainer exist, which integrate with the Docker engine and allow for a more robust and user-friendly way to manage a Docker installation.\nEnough intro, let\u0026rsquo;s get started!\nRequirements Basic understanding of virtualization An operating system which supports Docker A willingness to learn and make mistakes! Installation We\u0026rsquo;ll keep this section simple - follow the official installation documents for whichever operating system you are using. The author of this document is using Linux, so any included screenshots will be from a Linux terminal.\nBasics \u0026amp; Methodology Docker is a containerization engine and platform. What does this mean? Rather than needing to install an entire operating system for a specific application and manage all the dependencies (specific programming libraries, security updates, version updates, etc.), I can instead package only what is needed to keep that application running and place it in a \u0026ldquo;container\u0026rdquo;. This way, I can tightly control what dependencies exist and ensure things don\u0026rsquo;t break.\nIn addition, just like a shipping container can fit on a semi truck (or lorry, for the folks not based in the US) or a large ship, a Docker container can be packaged and run on any system that has Docker installed. And since Docker is cross-platform, this ensures ultimate compatibility across all operating systems.\nDocker Structure Docker is organized in a hierarchical fashion:\nA Dockerfile is used to specify what commands to run, what to install, etc. This is exactly like what would be done via the command line but is instead in a text file A Docker image is then constructed from the Dockerfile. The images are built in layers, with each command inside the Dockerfile resulting in a new image layer. A downside to this layer system is you have to structure your Dockerfile to make sense with how the layers will operate - layers are constructed into the image and then deleted, so you have to plan how the commands will link together. You may not have access to a previous layers\u0026rsquo; output without proper planning. Images can be tagged to differentiate versions, i.e. using \u0026lt;imagename\u0026gt;:\u0026lt;v1.0\u0026gt; instead of the default of \u0026lt;latest\u0026gt;. One or more Docker containers can then be created from an image Unless otherwise configured, Docker containers are ephemeral - changes you make won\u0026rsquo;t survive a reboot of the container. To the point above, Docker containers don\u0026rsquo;t have access to storage or other persistent settings unless explicitly defined. The VOLUME option is used to specify the mount points that will be persistently available. Unless otherwise specified, containers are ephemeral, or designed to be operated in a \u0026ldquo;run and done\u0026rdquo; type fashion - that is, a container is started, executes a command (defined in the CMD portion of it\u0026rsquo;s Dockerfile) and then exits. Given the hierarchical nature, it\u0026rsquo;s important to note that an image cannot be deleted if a container is using it. Containers need to be stopped and deleted in order to remove an image that is dependent on it. This could be forced but probably isn\u0026rsquo;t a good idea in a production environment Basic command cheatsheet Let\u0026rsquo;s summarize some basic commands that are commonly used with Docker. These are especially helpful for those managing a Docker installation from a CLI (as opposed to using Portianer or other products, such as Synology, which integrate Docker and provide a GUI for management).\n"
},
{
	"uri": "http://localhost/tags/easy/",
	"title": "Easy Tag",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/explanations/",
	"title": "Explanations",
	"tags": [],
	"description": "",
	"content": "Find explanations and definitions for common terms and concepts related to self-hosting Topics Explanations:\nServers Daemons Webservers Domain names DNS Reverse proxies Port forwarding Containers Virtualization Virtual private networks Operating systems Servers Servers are machines whose purpose is to provide a service or content over a network. They are typically administered remotely and only connect physically to power and a network. They \u0026ldquo;serve\u0026rdquo; content or services using software daemons.\nBare metal servers are not virtualized. Any service or content they offer is configured on the host system. They are not new per-se, but with the introduction of containerization and virtualization, the phrase has been coined to differentiate the old-school server tradition from newer techniques.\nTheir natural habitat is the datacenter, where they live in racks to survive off electricity and network data. While they are not able to reproduce, they have no natural predators, so their population is stable. Some breeds of server can be found in network/data closets where they live in a business. Fewer are still kept in captivity in private homes.\nVirtual servers are servers that are run under an emulator or hypervisor to provide a server-like environment using a software envelope which may be augmented with hardware support.\nTop\nDaemons Daemons are software packages that run perpetually to provide content or a service. They differentiate servers from clients.\nExamples of daemons are webservers, email servers, file servers, authentication services (AD, LDAP), database servers, and many more.\nTop\nWebservers Webservers are daemons that accept HTTP requests and serve set content based on the requested host (IP address or domain name).\nThe content can be static HTML/XML or it can be dynamic (JavaScript, PHP, FCGI, WSGI).\nWebservers commonly offer reverse proxy functionality, it is common to use webservers for this purpose instead.\nCommon webservers are: Apache, Cherokee, LiteSpeed, Lighttpd, nginx, and IIS. Apache and nginx are the top webservers by market share respectively, with IIS coming in third.\nTop\nDomain names Domain names are a word, phrase, or string that is used for navigating the Internet. They are registered to individuals or legal entities in lengths of years for a set fee.\nThey are divided into levels, where each level is separated by a period (dot). Domain registrations include the top-level and second-level portion of a domain. All levels below are controlled by DNS at the discretion of the domain registrant.\nTop-level domains (TLDs) are .com, .net, .info, .edu, .org, etc.\nThe customizable part of the domain name you can register is called the second-level domain.\nThird-level domains are referred to as subdomains.\nStructure: subdomain.secondleveldomain.tld\nE.g.: wiki.r-selfhosted.com\nDomain registration Registering a domain name is done with a Domain Registrar. Prices are based on the top-level domain, but all registrations are for a period of one year minimum.\nRegistrars come in two flavors:\nAccredited: These registrars work directly with ICANN or other regional Internet registries for domain registrations. Domain Resellers: These companies work with a \u0026ldquo;white label\u0026rdquo; registrar to resell domain registrations for a small markup. Accreditation requires quite a bit of infrastructure and vetting to make sure you can handle all aspects of registering and maintaining domains on behalf of the registrant.\nResellers are popular because of low overhead and easy implementation. Many \u0026ldquo;white label\u0026rdquo; registrars have turnkey solutions for resellers to appear as independent registrars while actually reselling domain names.\nWhich is right for you? Choosing a domain registrar is easy. Picking a domain registrar that is trustworthy and reputable is less so.\nMany domain registrars also offer to handle the DNS records for the domains registered with them. Many registrars have domain registration as a part of their business. Registration is usually bundled with webhosting or other related services. You may even get a domain registration for free if you agree to a year-long hosting contract with a webhost.\nWhile bundling related services together under one roof may sound convenient, it is generally not a good idea. It is recommended to have your domain registration with a registrar, DNS records with another company, and hosting with yet another entity. Common reasoning for this piece of advice is that if your service provider has a serious outage or other technical problem, it can only affect one aspect of your online presence. If you have all services under one provider, a technical issue could prevent your DNS from resolving and your website/service from being served.\nYou can find the list of ICANN-accredited domain registrars here: https://www.icann.org/registrar-reports/accreditation-qualified-list.html\nAs far as finding a reputable, trustworthy service provider, we must insist on your own research. One of the most popular forums for discussing hosting and related services is Web Hosting Talk. If a relevant service provider has a bad reputation in the industry, you can surely find out about it here.\nTop\nDomain Name System The Domain Name System (DNS) is the method of defining what unique machines serve content for your domain.\nThe important parts of DNS you have to worry about are nameservers and DNS records.\nNameservers Nameservers are a way to declare which servers are responsible for answering record requests for your domain. Most registrars provide DNS services, but if you have your DNS provided elsewhere, you will want to provide your primary and secondary nameservers to your registrar. The nameservers to use will be provided by your DNS service provider.\nE.g.: ns1.dnsnameserver.net, ns2.dnsnameserver.net\nDNS records DNS records are part of your domain name configuration called a DNS zone.\nSOA: Start of Authority Records is generally handled by your DNS service provider automatically. They define:\nMNAME: Master nameserver for the zone. RNAME: Email for the domain administrator. Does not support \u0026ldquo;@\u0026rdquo;, use periods. Periods before the domain name are escaped. E.g.: some.one@example.com =\u0026gt; \u0026ldquo;some\\.one.example.com\u0026rdquo;. SERIAL: The DNS zone serial, used to indicate when a zone has changed. REFRESH: Time to wait for secondary nameservers to query the master. RETRY: Timeout for refreshing. EXPIRE Threshold time for secondary nameservers to stop attempting to reach an unresponsive master server. TTL: The time to live to use for NXDOMAIN responses. Example SOA:\n$TTL 86400 @ IN SOA ns1.nameserver.com. postmaster.sumdomain.com. ( 2020080302 ;Serial 7200 ;Refresh 3600 ;Retry 1209600 ;Expire 3600 ;Negative response caching TTL ) The fields of a DNS zone record are:\nDomain: Either the domain name or subdomain to create a record for. Time to live: The time in seconds for a record to be cached before a new copy is requested. Class: Indicative of the namespace. Usually IN (Internet namespace). Type: The type of record to define. Content: The content of the record. What is acceptable in this field is dependent on the type of record. There are many types of DNS records, let\u0026rsquo;s go over some common ones. This list is not exhaustive.\nA: A records tie the domain or subdomain to an IPv4 address. AAAA: AAAA records tie the domain or subdomain to an IPv6 address. CNAME: CNAME records tie the domain or subdomain to another domain or subdomain. MX: MX records are used to define how mail is handled for your domain. The content of an MX record is the priority and answering server domain name. Lower preference number indicates higher priority. TXT: Text records associate text data with your domain. They are used for a variety of reasons, notable for SPF or DKIM. Domain Time To Live Class Type Content example.com. 86400 IN A 192.168.1.240 ipv6.example.com. 86400 IN AAAA feef:00bb:2005:1eef:fbca:544d www.example.com. 86400 IN CNAME example.com. example.com. 86400 IN MX 10 mail.mailserver.com example.com. 86400 IN TXT \u0026ldquo;Reserved for a purpose I am not legally required to disclose.\u0026rdquo; Top\nReverse proxies Reverse proxies are daemons that accept connections and then connect to another service based on port or host to facilitate the request. They act as a middleman instead of a traffic redirector.\nTypical use cases for reverse proxies are to provide a unified frontend for multiple backends or hosts. Another common use is for high-availability to provide failover or distribute load between multiple backends serving the same content.\nExamples of popular software capable of performing as a reverse proxy are: Squid, HAProxy, Apache, nginx, traefik, Nginx Proxy Manager (NPM) and Caddy.\nTop\nPort forwarding Port forwarding is the function of inspecting traffic on an incoming port and redirecting it to another port or host with minimal modification. Primary purposes of this are to forward traffic to a service behind a firewall/router.\nCommon for hosting game servers from home when running dedicated servers before developers moved to match-making. Another use for this is to open ports for BitTorrent so that you can share your vast and innumerable collection of Linux ISOs.\nThe difference between port forwarding and a reverse proxy is that the reverse proxy will accept, process, and establish a new connection to the backend service to fulfil the request.\nPort forwarding inspects and alters packet headers before it is routed to its new destination. The connection is otherwise untouched.\nPort forwarding is a function of your firewall. Commonly at the router or other network gateway.\nLinux has one firewall called iptables with many frontends or management packages available for it. BSD-based firewalls are pf, ipfw, and IPFilter. The Windows firewall consists of a scarecrow holding a sign saying: \u0026ldquo;plz no tresspass\u0026rdquo;.\nTop\nContainers Containers are software envelopes to isolate a piece or bundle of software and their dependencies. Containers come in many forms.\nA container could contain a PHP-based forum with an AMP stack (Apache, Maria DB, PHP) as dependencies.\nThis is useful if you want an easy way to deploy software without configuring dependent software/libraries manually.\nContainers can also resolve software conflicts when running multiple services which depend on different versions of the same software/libraries.\nPopular containers are Linux Containers (LXC), jails (BSD UNIX), Kubernetes, and Docker.\nTop\nVirtualization Virtualization is a lower level form of containerization. There are many forms of virtualization that provide different sets of features/tradeoffs.\nIn practice, it often virtualizes whole or major parts of an operating system. Virtualization also requires significantly more resources than a container, and can either be on an empty (bare metal) server or may be nested, such a VMWare Workstation/Virtualbox.\nFull virtualization Full virtualization is generally the containerization of a full, unmodified operating system with virtualized hardware. The virtualized OS is not host-aware.\nFully virtualized guests require more overhead than paravirtualized guests. This can be mitigated with hardware support (Intel VT, AMD SVM) for virtualization instructions.\nExamples of this are Hyper-V, Xen, KVM/Qemu, VMware ESXi.\nParavirtualization Paravirtualization is the practice of running a modified kernel/OS where privileged instructions are sent through an API shared with the host. It does not require the virtualization of hardware, but it does require an operating system that is modified to be used with the specific API used by your chosen virtualization method. This can be in the form of source code modifications or specialized device drivers.\nMicrosoft Windows cannot be paravirtualized.\nExamples: Xen, Oracle VM, OpenVZ.\nTop\nVirtual private networks Virtual private networks (VPNs) are a way of networking individual machines together in software regardless of their physical or network proximity.\nA typical use case is for networking corporate locations together to share network resources such as file shares, intranet webservers, on-premises services, etc.\nAnother use for a VPN is to tunnel traffic destined for a public service through to another endpoint, usually to bypass geo-location restrictions or state-imposed censorship of the Internet.\nSome use VPNs to keep services behind a restrictive ISP or firewall accessible outside of said network.\nTop\nOperating systems An operating system is the software that is responsible for running and managing your physical machine. It provides the kernel, hardware drivers, low-level software packages, libraries, and userland applications for the end-user to provide basic functions.\nEnd-user operating system for desktop computers or laptops is typically Windows.\nCorporate IT infrastructure to serve employees is generally Microsoft-based additionally using AD for authentication, on-premises exchange, IIS webservers, MS SQL databases, and other service needs.\nThe operating system used to serve content and services to end-customers is typically Linux or UNIX-based. Examples are webhosts for serving websites, Netflix for serving movie and TV streams, DNS services for domain records, and most if not all other infrastructure needed to keep the Internet operational.\nAndroid is a Linux-based operating system used in the majority of the smartphone market. And iOS is a UNIX-based mobile OS used by Apple for iPhones.\nEmbedded Linux and BSD are also used in devices like set-top boxes, smart TVs, routers, smart switches, medical equipment, flight telemetry controllers for aerospace, navigation equipment, industrial automation, etc.\nTop\n"
},
{
	"uri": "http://localhost/contribution/file-structure/",
	"title": "File Structure",
	"tags": [],
	"description": "",
	"content": "Hugo\u0026rsquo;s file structure is simple. For example, here is the part of the file structure of this very wiki.\n/ content _index.md / Contribution _index.md / Getting Started _index.md / Adding a Page _index.md / Guides _index.md / Webservers _index.md "
},
{
	"uri": "http://localhost/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Here are some explanations for the basics and themes of Self-Hosted.\nWhat is self-hosting? How do I self-host? What are some self-hosted alternatives to stuff I use every day? Difficulty tiers Intro to reverse proxies Operating systems "
},
{
	"uri": "http://localhost/guides/devops-toolchains/gitlab-kubernetes/",
	"title": "Gitlab Installation",
	"tags": ["normal"],
	"description": "",
	"content": " Introduction In this article, I will describe all the steps required to setup GitLab CI/CD in kuberntes using kustomize. We will go through how to run GitLab on Kubernetes when you have related resources postgres, redis, minio, tls certificates etc already available in your setup.\nThis is a very common scenario in companies and also for self-hosting that you are already using these services in your environment and prefer to use the same for gitlab.\nThe all in one production installation may be easily performed with Helm. You can refer to official documentation from gitlab if that is your requirement.\nRequirements You will need the following in order to run gitlab.\nDatabase : Postgres database is required for gitlab. Cache : Redis is used for caching. Storage : Minio is used as object storage for container registry, gitlab backups, terraform storage backend, gitlab artifacts etc. Ingress Controller : Nginx ingress is part of installation. Persistent Volume : Gitaly will store repository data data on disk, for that your kubernetes cluster must have a way of provisioning storage. You can install local path provisioner in your cluster for dynamically provisioning volumes. Repositories Gitlab Manifests SubVars App Info: You can swap minio with any other object storage i.e S3 by changing connection info secret\nLets get Started When installing gitlab with helm it generates the configmaps after rendering the templates with parameters, we can manually change these values in configmaps but its a hassle and not convenient.\nTo make this process easy we will use a tool called subvars which will let us render these values from command line. Install it by following the instructions on github page, we will use it later.\nDownload the release with manifests from github alternatively you can clone the repo, if you are cloning the repo remove the .git folder afterwards as it creates issues some times when rendering multiple version of the same file with subvars. export RELEASE_VER=1.0 wget -q https://github.com/kha7iq/gitlab-k8s/archive/refs/tags/v${RELEASE_VER}.tar.gz tar -xf v${RELEASE_VER}.tar.gz cd gitlab-k8s-${RELEASE_VER} Lets start by setting the url for gitlab in our kustomization file within ingress-nginx folder. You will find two blocks, one for web-ui and second for registry along with tls-secret-name for https. patch: |- - op: replace path: /spec/rules/0/host value: your-gitlab-url.example.com - op: replace path: /spec/tls/0/hosts/0 value: your-gitlab-url.example.com - op: replace path: /spec/tls/0/secretName value: example-com-wildcard-secret We can create minio-conn-secret containing configuration for minio. It will be used for all the enabled S3 buckets except gitlab backups, we will create that separately. Input the information as per your setup and create the kubernetes secret. minio.config cat \u0026lt;\u0026lt; EOF \u0026gt; minio.config provider: AWS region: us-east-1 aws_access_key_id: 4wsd6c468c0974006d aws_secret_access_key: 5d5e6c468c0974006cdb41bc4ac2ba0d aws_signature_version: 4 host: minio.example.com endpoint: \u0026#34;https://minio.example.com\u0026#34; path_style: true EOF Kubernetes secret kubectl create secret generic minio-conn-secret \\ --from-file=connection=minio.config --dry-run=client -o yaml \u0026gt;minio-connection-secret.yml Next step is to create a secret with mino configuration for gitlab backup storage. Just replace minio endpoint, bucket name, access key \u0026amp; secret key. cat \u0026lt;\u0026lt; EOF \u0026gt; storage.config [default] access_key = be59435b326e8b0eaa secret_key = 6e0a10bd2253910e1657a21fd1690088 bucket_location = us-east-1 host_base = https://minio.example.com host_bucket = https://minio.example.com/gitlab-backups use_https = True default_mime_type = binary/octet-stream enable_multipart = True multipart_max_chunks = 10000 multipart_chunk_size_mb = 128 recursive = True recv_chunk = 65536 send_chunk = 65536 server_side_encryption = False signature_v2 = True socket_timeout = 300 use_mime_magic = False verbosity = WARNING website_endpoint = https://minio.example.com EOF kubectl create secret generic storage-config --from-file=config=storage.config \\ --dry-run=client -o yaml \u0026gt; secrets/storage-config.yml All other secrets can be used as is from repository or you can change all of them following gitlab documentation\nOne of the most important secret is gitlab-rails-secret, in case of a disaster where you have to restore gitlab from a backup you must apply the same secret in your cluster as these keys will be used to decrypt the database etc from backup. Make sure you keep this consistent after first install.\nWe reached the last part, Its alot of work to change database and other parameters one by one in configmaps. I have implemented some templating for this which can provide all the values vi environment variables and render the manifests with subvars, it will output these to destination folder and replace all the parameters defined as go templates. The values are self explanatory, GITLAB_GITALY_STORAGE_SIZE variable is used to specify how much storage is needed for gitaly and GITLAB_STORAGE_CLASS is the name of storage class.\nGITLAB_URL=gitlab.example.com \\ GITLAB_REGISTRY_URL=registry.example.com \\ GITLAB_PAGES_URL=pages.example.com \\ GITLAB_POSTGRES_HOST=192.168.1.90 \\ GITLAB_POSTGRES_PORT=5432 \\ GITLAB_POSTGRES_USER=gitlab \\ GITLAB_POSTGRES_DB_NAME=gitlabhq_production \\ GITLAB_REDIS_HOST=192.168.1.91:6379 \\ GITLAB_GITALY_STORAGE_SIZE=15Gi \\ GITLAB_STORAGE_CLASS=local-path \\ subvars dir --input gitlab-k8s-1.0 --out dirName Change into dirName/gitlab-k8s-1.0 you can have a look to confirm if everything is in order before applying this in cluster.\nThe final step is to create the namespace gitlab and build with kustomize or kubectl. I prefer kustomize but you can also use kubectl with -k flag. Create namespace kubectl create namespace gitlab Apply the final manifest kustomize build gitlab-k8s-1.0/ | kubectl apply -f - # or following if you have already changed into directory kustomize build . | kubectl apply -f - # With kubectl kubectl apply -k gitlab-k8s-1.0/ # or following if you have already changed into directory kubectl apply -k . Head over to the endpoint you have configured for gitlab https://gitlab.example.com and login. Note: Default passwords Gitlab \u0026lsquo;root\u0026rsquo; user\u0026rsquo;s password configured as secret\nLAwGTzCebner4Kvd23UMGEOFoGAgEHYDszrsSPfAp6lCW15S4fbvrVrubWsua9PI Postgres password configured as secret\nZDVhZDgxNWY2NmMzODAwMTliYjdkYjQxNWEwY2UwZGMK "
},
{
	"uri": "http://localhost/guides/",
	"title": "Guides",
	"tags": [],
	"description": "",
	"content": " Find the link to your desired guide below!\nReverse proxies nginx Virtual private networks WireGuard DevOps Toolchains Gitlab Installation on Kuberntes "
},
{
	"uri": "http://localhost/tags/hard/",
	"title": "Hard Tag",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/getting-started/how-to-self-host/",
	"title": "How to Self-Host",
	"tags": [],
	"description": "",
	"content": "Here we will go over the basics of what self-hosting entails. There will be a lack of detail in each individual step as those details are reserved for their own documentation in a separate article or are specific to a provider/service as not everything can be self-hosted, especially if you wish to make a service publicly available.\nContents Choosing a server Domain name registration DNS configuration Network configuration Server setup Choosing a server A server can be just about anything. An old computer that you don\u0026rsquo;t use anymore, a cheap small form factor machine from an online store, a custom built machine using server grade or consumer parts, or even a Raspberry Pi which can be gotten for under $100.\nAn operating system to install to make it a server is another choice you have to make. Windows Server is usually not chosen due to licensing costs, but it is an option. Linux comes in many flavors to meet many needs. It is freely available and free to modify.\nTo the top\nRegistering a domain name Registering a domain name is usually the first step when you are looking to have something self-hosted that is reachable via the Internet. Although you can even find a free domain name, it is always a good idea to find a domain registrar you can trust; recommending specific providers is outside the scope of this wiki.\nTop-level domain names vary in price. Dot.com domain names are usually ~$15/yr, while others like dot.net or dot.info are typically less. Some can be more expensive, and the registrar will have their price listings based on the top-level domain. Domain registration is a competitive business, with lots of resellers working for white-label registrars. You can look at several registrars to compare prices for the domain you want to register. In some cases you may choose a free domain name, e.g. under .TK, .ML, .GA, .CF, or .GQ, but be ready that most search engines would range your site much worse, so such solution is not recommended for serious projects.\nDomain registrations are on a first-come, first-serve basis. If the domain name you want is already taken, you will have to wait until it expires, or buy it from the current owner. Otherwise, you can come up with a different domain name to register.\nBEWARE: Some domain registrars will take domain names that you search for, but not purchase, and \u0026ldquo;hold\u0026rdquo; their registration.\nE.g.: You go to someregistrar.com and search for ilikeponies.com to see if it\u0026rsquo;s available for registration. You find that it is available, but want to check out someotherregistrar.com to see their prices for the same domain. When you check there, the domain ilikeponies.com is taken. But when you search for it again on someregistrar.com, it\u0026rsquo;s still available.\nWhat is happening is that someregistrar.com has \u0026ldquo;held\u0026rdquo; the registration for a period of time to prevent you from registering it elsewhere in hopes that you take your business with them.\nWhile we are not certain of the legality of such practice, it is considered uncouth and frowned upon in the industry. You can resolve this by contacting the registrar, telling them they are massive jerks, and have them release the hold.\nIf they have no shame, you may have to explain that your poor experience with them can wind up on webhostingtalk.com for the world to see.\nTo the top\nDomain name records Once you have registered a domain name, you will need to configure the nameservers and then add DNS records for the domain that points to the IP address of your server.\nYour domain registrar makes use of the nameserver records, and your DNS provider will take care of the other types of records.\nThe basic DNS records to add are an A record for yourdomainname.com, and a CNAME for www.yourdomainname.com.\nGet the IP address of the machine you will use. If you serve locally, then the local IP address of the machine will not work (e.g.: 192.168.1.X). You will have to use your public IP and use a reverse proxy + VPN or port forward. You can find your public IP address on your router status page or by going to https://ifconfig.me/.\nUse that IP as the content for your DNS A record.\nSee your router documentation for directions on setting up port forwarding to the machine\u0026rsquo;s local IP address.\nTo the top\nNetwork configuration Your ISP may block inbound traffic on common ports used for Internet traffic such as 80 (HTTP), 443 (HTTPS), 21 (FTP), 25 (email SMTP), and many others.\nThere may be clauses in your service agreement stating that hosting services from your home Internet connection is prohibited. While ISPs rarely take action against customers if they are found in violation of this rule, there are legitimate reasons for having such clause.\nGenerally if you bypass their restrictions to self-host anyway, they don\u0026rsquo;t care unless you cause problems for them.\nIf you have a business Internet plan, these issues don\u0026rsquo;t exist. You may still have to request a static IP, but your ISP should not be blocking ports by default as it is assumed you will be handling that yourself. If they are, they should cooperate with any unblocking requests.\nThe typical way to bypass these restrictions is either to port forward (if your ISP doesn\u0026rsquo;t block the ports) or using a reverse proxy in conjunction with a VPN using an Internet-reachable machine.\nYou also have to contend with dynamic IP addresses. Your home connection is assigned to a public IP address, but it is not fixed, and can change if your DHCP lease on their network expires. Some ISPs may offer to provide you a static IP address for a monthly fee.\nFor dynamic IP addresses, your DNS provider may support dynamic DNS updates. This can either require a dedicated client, or to simply call a URL with an embedded token with a program like Wget or cURL. It is an effective way to have a DNS record automatically updated on a regular basis to counteract a changing IP address.\nTo the top\nServer configuration You need a server to handle requests for your domain name. This can be a shared hosting account with a web hosting provider, a dedicated server, a virtual private server, or your own server running locally.\nDespite the fact that you might find providers that could serve you some limited server storage and computing power remotely even for free, this is a self-hosting guide, so the assumption that the server is local has been made.\nOnce you have chosen and installed the operating system you are going to use, you will need to setup your service daemon. For the sake of this article, we will assume that you\u0026rsquo;d like to serve a website using your domain name.\nThere are a variety of web servers available, and the specific details on configuring that web server will be covered elsewhere. The basic steps are:\ninstall the web server configure it to answer for your domain name give it a directory to serve from (/some/directory/domainname.com/html) upload content into that folder Make sure the content folder has the permissions needed by your web server, and you should be good to go.\nThis same process is generally the same for any service, but the documentation for said service should be consulted.\nTo the top\n"
},
{
	"uri": "http://localhost/getting-started/reverse-proxies/",
	"title": "Intro to Reverse Proxies",
	"tags": [],
	"description": "",
	"content": "Reverse proxies and YOU What can a reverse proxy do for you?\nMany services that are self-hosted have a web UI. If you have many of these services running, a reverse proxy can be a central daemon that handles requests for all of these various backends.\nLet\u0026rsquo;s say that you have three different services running:\nNode.js forum NodeBB that runs on port 8008 Navidrome music streaming server on port 3000 through a local machine using IP 192.168.1.125 ownCloud web UI on port 8800 on a local machine with an IP of 192.168.1.123 You have a domain name that you want to use for these but don\u0026rsquo;t want an ugly URL like http://mydomain.com:8008. To complicate things further, these services are all on different hosts.\nA reverse proxy can be configured to accept requests for this domain and redirect them to a different host or port.\nTo make your URLs pretty, the reverse proxy can be configured to redirect requests on your domain based on a folder name to a different service on your local network.\nhttp://mydomain.com/forum/ \u0026ndash;\u0026gt; http://localhost:8008/ http://mydomain.com/music/ \u0026ndash;\u0026gt; http://192.168.1.125:3000/ http://mydomain.com/cloud/ \u0026ndash;\u0026gt; http://192.168.1.123:8800/ As you can see, it is much nicer to reach these services through a single domain and folder than to use their port and host individually.\nYou can even couple this with a self-hosted VPN so that these requests can be proxied to different services on different networks in different locations. All you need to do is to make sure the proxy and the services are on the same VPN and to use the VPN IP addresses.\nYou don\u0026rsquo;t have to use folders either. You can use subdomains as well such as music.mydomain.com, cloud.mydomain.com, and forum.mydomain.com respectively. It\u0026rsquo;s all up to you and how you want to structure your services.\nPopular Reverse Proxies This wiki already contains steps on how to set up nginx, which is one of the more popular reverse proxies and may be encountered in enterprise environments, however it is not the only option. Some other options include:\nNginx Proxy Manager (GUI front-end to NGINX) Caddy (can be used as both an HTTP server or as a reverse proxy, so it is included here as well) Traefik The general concept of these is the same as mentioned above - they allow you to host multiple services from behind a single IP address, they allow for flexibility and control and, most importantly, they offer security by handling SSL for you. A brief list of features to help you decide:\nGraphical interface File-based configuration Handle SSL automatically Route traffic based on labels Used in Enterprise Deployments Allow configuration of subdomains and subdirectories Easy to read configuration Integrate directly into docker compose files NPM X X X Traefik X X X X X X Caddy X X X X Pure NGINX X X X X "
},
{
	"uri": "http://localhost/contribution/modify-a-page/",
	"title": "Modify a Page",
	"tags": [],
	"description": "",
	"content": "Modifying a page is as simple as changing the _index.md file inside a folder.\n\\ content _index.md \u0026lt;-- this here \\ Subfolder _index.md \u0026lt;-- or this one \\ Subfolder2 _index.md \u0026lt;-- or even this one too Hugo uses Markdown to format and style text, among other things.\nYou can find more information on how to use Markdown Syntax here: https://daringfireball.net/projects/markdown/basics\n"
},
{
	"uri": "http://localhost/guides/reverse-proxy-servers/nginx/",
	"title": "Nginx Reverse Proxy",
	"tags": ["normal"],
	"description": "",
	"content": " What is a reverse proxy? To learn what a reverse proxy is and why you should use one, read the explanation here.\nPrerequisites There are some prerequisites you\u0026rsquo;ll need before setting up a reverse proxy server. The first thing you\u0026rsquo;ll need is to have port 80 and 443 of your public IP address forwarded to the machine you want to use as a proxy. This can be configured through your router\u0026rsquo;s admin page. You will also need a domain name with an A record that points to your public IP. Finally, you\u0026rsquo;ll need some services running on your local network for you to proxy.\nnginx installation Debian-based systems First, type sudo apt update to update the package information. Then, type sudo apt install nginx to install nginx. Finally, allow the necessary ports using sudo ufw allow 80/tcp and sudo ufw allow 443/tcp.\nRHEL-based systems First, enable the EPEL repository using sudo yum install epel-release. Then, type sudo yum install nginx to install nginx. Finally, allow the necessary ports using sudo firewall-cmd --permanent --zone=public --add-service=http and sudo firewall-cmd --permanent --zone=public --add-service=https. Also, type sudo firewall-cmd --reload to reload the firewall.\nMake sure nginx starts up using sudo systemctl start nginx.\nTo verify that nginx is working properly, visit http://yoursite.com and you should see a nginx welcome page similar to what\u0026rsquo;s shown below. This specific page may vary depending on your distro.\nDeciding the reverse proxy structure Before we actually create our reverse proxy configuration, we have to decide which local servers will handle each of the subdomains. For example, if I wanted nextcloud.yoursite.com to be handled by a server at 192.168.0.230, I could add a nginx configuration for that.\nOnce you\u0026rsquo;ve decided which subdomains you\u0026rsquo;ll use, add DNS CNAME records that map the subdomain to your main domain name. Below is an example in Google Domains, but it will vary depending on your DNS provider.\nModifying the configuration files Note: Editable templates for each of the config files shown in this guide can be found at this GitHub repo.\nIn order to set up the reverse proxy, we have to remove the default website and add our own configuration to handle each subdomain. In this guide, we\u0026rsquo;ll create two config files, one for a www/non-www domain and one for any other subdomain.\nRemoving the default configuration To remove the default configuration, we can type cd /etc/nginx/sites-enabled/ to enter the directory and sudo rm default to remove the config file.\nCreating the first config file To begin, type cd /etc/nginx/sites-available/ to enter the sites-available directory. Then type sudo vi reverse-proxy.conf to begin editing the file.\nThe first thing you\u0026rsquo;ll want to add in this file is a server block. This server block will listen on http://www.yoursite.com and redirect visitors to https://www.yoursite.com.\nserver { listen 80; server_name www.yoursite.com; return 301 https://www.yoursite.com$request_uri; } The next thing to add is another server block, which will listen on http://yoursite.com and redirect visitors to https://www.yoursite.com\nserver { listen 80; server_name yoursite.com; return 301 https://www.yoursite.com$request_uri; } Our third server block will listen on https://yoursite.com, and redirect the https traffic to https://www.yoursite.com. This server block also contains information about the SSL certificates, which we will modify later when we obtain them.\nserver { listen 443; server_name yoursite.com; return 301 https://www.yoursite.com$request_uri; # SSL Configuration ssl_certificate /etc/letsencrypt/live/yoursite.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/yoursite.com/privkey.pem; # managed by Certbot ssl on; ssl_session_cache builtin:1000 shared:SSL:10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!eNULL:!EXPORT:!CAMELLIA:!DES:!MD5:!PSK:!RC4; ssl_prefer_server_ciphers on; } This last server block will perform the actual proxying. It will listen on https://www.yoursite.com and proxy requests to a backend server. To do this, we can add a location block within this server block. Within the location block, we set proxy headers which nginx forwards to the backend, and we add the proxy pass and proxy redirect with the IP address and port of the backend server. The last few lines are optional, but I recommend using them because they heighten the security of your server. These lines enable HSTS, clickjacking protection, XSS protection, and disable content and MIME sniffing. Finally, we can add a line which adds the trailing slash to all URLs.\nserver { listen 443; server_name www.yoursite.com; # SSL Configuration ssl_certificate /etc/letsencrypt/live/yoursite.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/yoursite.com/privkey.pem; # managed by Certbot ssl on; ssl_session_cache builtin:1000 shared:SSL:10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!eNULL:!EXPORT:!CAMELLIA:!DES:!MD5:!PSK:!RC4; ssl_prefer_server_ciphers on; # Set the access log location access_log /var/log/nginx/yoursite.access.log; location / { # Set the proxy headers proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # Configure which address the request is proxied to proxy_pass http://yourserverip:yourport/; proxy_read_timeout 90; proxy_redirect http://yourserverip:yourport https://www.yoursite.com; # Security headers add_header Strict-Transport-Security \u0026#34;max-age=31536000; includeSubDomains; preload\u0026#34;; add_header X-Frame-Options DENY; add_header X-Content-Type-Options nosniff; add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34;; add_header Referrer-Policy \u0026#34;origin\u0026#34;; # Add the trailing slash rewrite ^([^.]*[^/])$ $1/ permanent; } } After adding these lines, type :wqa to save the file and exit Vim.\nThere\u0026rsquo;s one more step before we can use this config: symlinking it to the sites-enabled directory (which nginx reads). To do this, type sudo ln -s /etc/nginx/sites-available/reverse-proxy.conf /etc/nginx/sites-enabled/reverse-proxy.conf.\nCreating the second config file This next config file will serve as a template for any other subdomains you want to add to your reverse proxy. To begin making this config file, type cd /etc/nginx/sites-available/ and then sudo vi SUBDOMAIN.conf, replacing \u0026ldquo;SUBDOMAIN\u0026rdquo; with the subdomain you want to configure.\nThe first thing we\u0026rsquo;ll add in this file is a server block. This server block will listen on http://YOURSUBDOMAIN.YOURSITEDOMAIN.com and redirect visitors to https://YOURSUBDOMAIN.YOURSITEDOMAIN.com.\nserver { listen 80; server_name YOURSUBDOMAIN.YOURSITEDOMAIN.com; return 301 https://$host$request_uri; } This next server block will perform the actual proxying. It will listen on https://YOURSUBDOMAIN.YOURSITEDOMAIN.com and proxy requests to your backend server. To do this, we\u0026rsquo;ll add a location block inside the server block. Within the location block, we set proxy headers which nginx forwards to the backend, and we add the proxy pass and proxy redirect with the IP address and port of the backend server. Again, the security headers at the bottom are optional, but they will greatly improve the security of your server, so I recommend that you add them.\nserver { listen 443; server_name YOURSUBDOMAIN.YOURSITEDOMAIN.com; # SSL configuration ssl_certificate /etc/letsencrypt/live/YOURSUBDOMAIN.YOURSITEDOMAIN.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/YOURSUBDOMAIN.YOURSITEDOMAIN.com/privkey.pem; # managed by Certbot ssl on; ssl_session_cache builtin:1000 shared:SSL:10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!eNULL:!EXPORT:!CAMELLIA:!DES:!MD5:!PSK:!RC4; ssl_prefer_server_ciphers on; # Set the access log location access_log /var/log/nginx/YOURSUBDOMAIN.access.log; location / { # Set the proxy headers proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # Configure which address the request is proxied to proxy_pass http://YOURSERVER:YOURPORT; proxy_read_timeout 90; proxy_redirect http://YOURSERVER:YOURPORT https://YOURSUBDOMAIN.YOURSITEDOMAIN.com; # Set the security headers add_header Strict-Transport-Security \u0026#34;max-age=31536000; includeSubDomains; preload\u0026#34;; #HSTS add_header X-Frame-Options DENY; #Prevents clickjacking add_header X-Content-Type-Options nosniff; #Prevents MIME sniffing add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34;; #Prevents cross-site scripting attacks add_header Referrer-Policy \u0026#34;origin\u0026#34;; } } After adding these lines, type :wqa to save the file and exit Vim.\nFinally, to symlink this file to the sites-enabled directory, type sudo ln -s /etc/nginx/sites-available/SUBDOMAIN.conf /etc/nginx/sites-enabled/SUBDOMAIN.conf.\nTo add any additional subdomains, simply copy the previous config file and replace the server_name with the new subdomain, along with the backend\u0026rsquo;s IP address and port. Then symlink the new file to the sites-enabled directory.\nRestarting nginx If you try to restart nginx at this stage (sudo systemctl restart nginx), you\u0026rsquo;ll probably see a few errors saying that the certificate files don\u0026rsquo;t exist. In order to get nginx to start, we\u0026rsquo;ll have to use a temporary certificate.\nTo obtain a temporary certificate and store it in the working directory, type openssl req -newkey rsa:2048 -nodes -keyout key.pem -x509 -days 365 -out certificate.pem. Two files, key.pem and certificate.pem will be stored in your working directory.\nNow modify these two lines in your config files\nssl_certificate /etc/letsencrypt/live/yoursite.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/yoursite.com/privkey.pem; # managed by Certbot\nso that they look like this:\nssl_certificate /path/to/certificate.pem; # managed by Certbot ssl_certificate_key /path/to/key.pem; # managed by Certbot\nMake sure to replace /path/to/ with the path to your certificate and key files.\nAfter modifying these lines in each config, we can restart nginx using sudo systemctl restart nginx.\nObtaining Let\u0026rsquo;s Encrypt SSL certificates Now that nginx has restarted with the new configuration, we can obtain SSL certificates from Let\u0026rsquo;s Encrypt, a certificate authority that provides free certificates. To obtain a Let\u0026rsquo;s Encrypt certificate, we can use Certbot.\nTo install Certbot on a Debian-based distro, type sudo apt install python3-certbot-nginx.\nTo install on a RHEL-based distro, type sudo yum install certbot python3-certbot-nginx.\nThen, to obtain certificates for your www and non-www domains, type sudo certbot --nginx -d YOURSITEDOMAIN.com -d www.YOURSITEDOMAIN.com.\nCertbot will ask for some information, including your email address, agreement to the Terms of Service, and whether or not you want to subscribe to their newsletter. Then, Certbot will obtain your certificate.\nTo obtain a certificate for any additional subdomains, type sudo certbot --nginx -d sub.domain.com, replacing sub.domain.com with the proper subdomain address.\nCertbot will automatically update the config files with the path to your new certificates, so you don\u0026rsquo;t need to do that manually.\nOnce you\u0026rsquo;ve obtained all the certificates you need, restart nginx with sudo systemctl restart nginx.\nNow, visit each of your subdomains and ensure that they are accessible over https.\nAuto-renewal cron job The last thing we should do is to set up the auto-renewal of SSL certificates using cron. To do this, open the crontab for editing by typing sudo crontab -e.\nThen add the following line to the crontab to automatically try to renew the certificates at 1:00am every day:\n0 1 * * * certbot renew --deploy-hook \u0026quot;systemctl restart nginx\u0026quot;\nConclusion In conclusion, a reverse proxy allows you to easily host multiple sites on the same IP address without exposing unnecessary ports. If you enjoyed this article, feel free to check out my website, where I post articles about upgrading/restoring computers, securing your servers, and more. Thanks for reading and happy self-hosting!\n"
},
{
	"uri": "http://localhost/tags/normal/",
	"title": "Normal Tag",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/getting-started/operating-systems/",
	"title": "Operating Systems",
	"tags": [],
	"description": "",
	"content": "There are many operating systems available to meet different needs. Some are more difficult to administer than others, but there should be a solution available for whatever you want to do.\nWhile paying for a licensed product is always an option if you find the value in said product worth the cost, it is recommended to make sure that a \u0026ldquo;free\u0026rdquo; solution could not meet the same needs.\nProprietary / Licensed Microsoft Windows Server | Windows is not usually chosen due to licensing costs. Unraid | Linux-based, but requires the purchase of a license for usage past the trial period. Entry Level / Easy These are operating systems that are administered with a GUI/web-based frontend to focus on ease of use.\nYunoHost | Demo FreeNAS | A FreeBSD-based NAS operating system with ZFS support. OpenMediaVault DietPi | Built for single-board computers like Raspberry Pi or Pine64 boards. Sandstorm | Demo Intermediate / Headless If you are comfortable managing your server using terminal, then these options will work for you. They do a lot of hard work for you and should be simple to administer when needed.\nUbuntu Server | Debian-based server OS developed by Canonical Ltd. Rocky Linux | Fork/replacement for CentOS. openSUSE Fedora Linux Debian | One of the two oldest Linux distributions that are still maintained. Stable, mature, and proven. Advanced Arch Linux | A light, simple distribution that provides a small foundation to build on. Rich documentation and a large community-maintained third party software repository make it a solid choice for Linux veterans. Tailored for experienced users. Gentoo Linux | A Linux distribution focused on building packages from source to best fit your system. Binary packages are available, but that\u0026rsquo;s like, against the spirit duuude. Slackware Linux | The other of the two oldest Linux distributions that are still maintained today, it focuses on stability and sticking to its UNIX roots. FreeBSD | Almost as old as Linux itself, it is derived from BSD UNIX as developed at the University of California in Berkley. Used by Netflix as the OS powering its digital media delivery nodes. Alpine Linux | A tiny Linux distribution catering to power users who want to squeeze the most resources out of their systems. Niche / Other Proxmox VE | An operating system focused on the management of a virtualization environment utilizing KVM as a hypervisor. XCP-ng | XenServer-based, offers a turnkey virtualization solution. "
},
{
	"uri": "http://localhost/guides/reverse-proxy-servers/",
	"title": "Reverse Proxy Servers",
	"tags": [],
	"description": "",
	"content": "Reverse proxies are daemons that accept connections based on host or port, establish a connection to a backend service based on the connection request, and pass data between the client/backend.\nReverse proxy servers nginx "
},
{
	"uri": "http://localhost/getting-started/self-hosted-alternatives/",
	"title": "Self-Hosted Alternatives",
	"tags": [],
	"description": "",
	"content": "ALTERNATIVE STUFF HERE There will be some common choices in some common categories here for some self-hosted alternatives.\nA comprehensive, master list of self-hosted alternatives can be found at the following two repositories:\nSelf-Hosted Alternatives | Master List for self-hosted alternative software packages. Self-Hosted Sysadmin | Similar to the above, but oriented towards the needs of system administrators or IT professionals. None of the software listed on this page is endorsed or supported by r/selfhosted.\nWebservers Daemons that serve website content.\nApache | The most popular webserver since forever. Higher overhead than others, but most PHP applications assume it. nginx | Second most popular webserver today, created to run some of the biggest Russian websites. Lighttpd | A more niche webserver focusing on low overhead and high performance. Caddy | A fast, multi-platform web server with automatic HTTPS. Databases Daemons or services that store data in a structured format.\nPostgreSQL | A popular database solution that emphasizes extensibility and standards compliance. Maria DB | Based on MySQL, forked to maintain an open source alternative that is mostly compatible with MySQL-based applications. MongoDB | A document-based NoSQL database that uses JSON-like formatting to store information. SQLite | Flat file database that doesn\u0026rsquo;t require a running DB server. Content management systems Frontends for managing the content on your website.\nWordPress | The most popular CMS by market share, uses the blog format. Joomla! | Another popular CMS written in PHP. WonderCMS | Claims to be the smallest CMS around. Is definitely small. E-commerce Software for operating and managing an e-shop.\nOpenCart Magento PrestaShop File synchronization Services that synchronize files across systems.\nownCloud Syncthing Seafile Media streaming Daemons for streaming digital media.\nJellyfin | Media streaming server capable of handling audio, video, comics, books, and photos. Icecast | Operate your own Internet radio station! Navidrome | Music streaming software with a web UI and compatible with Subsonic/Airsonic clients. Photo galleries Software for operating a photo gallery.\nChevereto Zenphoto Piwigo Wiki software You own personal knowledge base!\nDokuWiki | A simple wiki that stores content in text files instead of a database. MediaWiki | The software that powers Wikipedia. Cowyo | A minimal wiki/note-taking package. "
},
{
	"uri": "http://localhost/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/guides/virtual-private-networks/",
	"title": "Virtual Private Networks",
	"tags": [],
	"description": "",
	"content": "Virtual private networks allow you to network any individual computers together into their own network in software. There are many options available.\nTraditional VPN solutions are L2TP/IPsec, OpenVPN, IKEv2 and PPTP. A more modern VPN software solution is WireGuard. You will find information on how to setup your own VPN in this section.\nVirtual private network software WireGuard "
},
{
	"uri": "http://localhost/guides/webservers/",
	"title": "Webservers",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/getting-started/what-is-self-hosting/",
	"title": "What is Self-Hosting?",
	"tags": [],
	"description": "",
	"content": "The act of providing or serving digital content or an online service typically delivered by a business.\nIt is generally served locally, as hosting it on a VPS or other Internet-residing machine is not \u0026ldquo;hosting\u0026rdquo; it yourself. There is still a middleman, and that is the owner/operator of that Internet-residing machine.\nOne of the easiest things to self-host with the lowest barrier to entry is a website. For the most basic website of your own, all you need is a domain name and a webserver. Then you throw a few lines of HTML in a file and you have yourself a \u0026ldquo;website\u0026rdquo;. With a service like Let\u0026rsquo;s Encrypt, securing the site with a SSL certificate is easy too.\nA lot of different services that you can self-host are \u0026ldquo;websites\u0026rdquo;. There are dynamic sites with robust content management systems like Joomla!, Drupal, WordPress, or b2evolution. There are forums like phpBB, MyBB, vBulletin, Discourse, etc. Knowledge bases like DokuWiki, MediaWiki, BookStack, or Gollum are also websites. These websites only require a webserver, an interpreter (PHP), and a database (SQLite, PostgreSQL, MySQL).\nJust about everything these days has a web UI or frontend to make things easier. HTTP/HTML/JS are well-understood standards that are ubiquitous. There are many libraries for converting or presenting your content in a web-friendly way for almost all programming languages you can learn.\nIt can be hard for someone unfamiliar to find the difference between the \u0026ldquo;website\u0026rdquo; frontend and the content backend. Sometimes the difference is almost non-existent. Sometimes there are many layers and systems working behind the scenes to make it happen.\nIt may be better to say that everything can be \u0026ldquo;accessed\u0026rdquo; through a website, even if it isn\u0026rsquo;t one per-se. And if it can\u0026rsquo;t, there\u0026rsquo;s probably a separate piece of software that makes a web UI for it.\nExamples of services with a web UI or separate web-based frontends are: BitTorrent clients like qBittorrent/Transmission, media streaming servers like Jellyfin/Navidrome, file synchronization services like Nextcloud/ownCloud/Seafile, communication services like Synapse/InspIRCd/jabberd/Mumble, and many more.\nOther services use the server-client model where the entire package is in two parts. The server part that runs at all time to serve content and the client part that connects to have content served to it. Examples are: game servers like Rust/Minecraft/Factorio, FTP servers, email servers, and more.\n"
},
{
	"uri": "http://localhost/guides/virtual-private-networks/wireguard/",
	"title": "WireGuard",
	"tags": ["normal"],
	"description": "",
	"content": "WireGuard is a secure VPN tunnel that aims to provide a VPN that is easy to use, fast, and with low overhead.\nIt is cross-platform, but it is the part of the Linux kernel by default with only the need of userland tools to configure and deploy it.\nPreface From now on, we are going to assume that we are working on Linux to configure WireGuard with one server and at least one client.\nThis guide assumes that this configuration is being performed as root or the superuser. For your distribution this may require you to prefix commands with \u0026lsquo;sudo\u0026rsquo;.\nInstallation You can find more details about installing WireGuard on your own operating system here: https://www.wireguard.com/install/. Please complete installation for both the server and client machine.\nMake the keys The first step after installing WireGuard for your distribution is to generate keys. We should do this for the server first, but this will be the same for clients as well.\ncd /etc/wireguard \u0026amp;\u0026amp; wg genkey | tee private.key | wg pubkey \u0026gt; public.key You should now have public.key and private.key files in /etc/wireguard/.\nIt is important to make sure your private key stays private. No private key should ever leave the machine it was generated on. The client and server will only need the public keys for each other. If you are using the private keys for a client on a server, or vice-versa, you are doing something wrong.\nServer configuration Since this is the server, we need to make a new configuration file for it in /etc/wireguard/. We will call it wg0.conf. The full path should end up being /etc/wireguard/wg0.conf.\nPlease use your own private key where appropriate. You can view the contents of a text file from the command line with cat (e.g.: cat /path/to/text.file).\nYou can change the Address field to use a different address space (e.g.: 192.168.x.1) if you wish. If your server or clients are already using private IP space on a LAN, use something different.\n[Interface] ## Private IP address for the server to use Address = 10.0.0.1/24 ## When WG is shutdown, flushes current running configuration to disk. Any changes made to the configuration before shutdown will be forgotten SaveConfig = true ## The port WG will listen on for incoming connections. 51194 is the default ListenPort = 51194 ## The server's private key. Not a file path, use the key file contents PrivateKey = PRIVATEKEY After this is done we should be able to start the VPN tunnel and make sure it\u0026rsquo;s enabled.\nPlease consult the documentation for your Linux distribution for enabling/starting services. This guide is using system tools installed on Debian and Debian-based distributions.\nDebian systemctl enable wg-quick@wg0 \u0026amp;\u0026amp; systemctl start wg-quick@wg0 That should be it for the server portion.\nClient configuration The client will need keys too. Use the same procedure to make keys for the client as we\u0026rsquo;ve done for the server.\nOnce that is done we need to create a client configuration. Let\u0026rsquo;s make wg0-client.conf in /etc/wireguard/. Full path should be /etc/wireguard/wg0-client.conf.\nYou will need to choose a unique IP for the client. Everything should be the same as the server\u0026rsquo;s private IP except the last octet.\n[Interface] ## This Desktop/client's private key ## PrivateKey = CLIENTPRIVATEKEY ## Client IP address ## Address = 10.0.0.CLIENTOCTET/32 [Peer] ## WG server public key ## PublicKey = WGSERVERPUBLICKEY ## set ACL ## ## Uncomment the next line to use VPN for VPN connections only # AllowedIPs = 10.0.0.0/24 ## If you want to use the VPN for ALL network traffic, uncomment the following line instead # AllowedIPs = 0.0.0.0/0 ## Your WG server's PUBLIC IPv4/IPv6 address and port ## Endpoint = WGSERVERPUBLICIP:51194 ## Key connection alive ## PersistentKeepalive = 20 This should be all you need for configuring the client-end connection. We will need the private client IP you\u0026rsquo;ve chosen and the public client key in a bit.\nAs with the server, we need to enable the WireGuard client service. We don\u0026rsquo;t start it yet because the server still doesn\u0026rsquo;t know about this client.\nDebian systemctl enable wg-quick@wg0-client Configuring the client as a peer Back on your server, we need to add the client so the server will accept the client connection. This is where your client private IP and public key will be used.\nRun the following command on the WG server to add the client.\nwg set wg0 peer CLIENTPUBLICKEY allowed-ips CLIENTPRIVATEIP/32 You should not need to restart the WireGuard service.\nLet\u0026rsquo;s start the WG client service on the client:\nDebian systemctl start wg-quick@wg0-client To check that it works, ping the WG server on its private IP.\n$ ping -c 1 10.0.0.1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=64 time=0.071 ms --- 10.0.0.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.071/0.071/0.071/0.000 ms If you consider your client Internet connection stable, this next step may not be necessary. You can consider yourself done if you wish.\nTHE END (maybe)\nWireGuard watchdog (OPTIONAL) Next we are going to setup a small cron job that will ping the WG server on its private IP to make sure the connection is still intact. If the connection fails, the tunnel will be restarted.\nYou can put this script anywhere, but I usually choose to put it in /usr/local/scripts/.\nmkdir /usr/local/scripts Now for the script. I use wg-watch.sh. Let\u0026rsquo;s assume you are going to use /usr/local/scripts/wg-watch.sh for the full file path.\n#!/usr/bin/bash # Modified from https://mullvad.net/en/help/running-wireguard-router/ # ping VPN gateway to test for connection # if no contact, restart! PING=/usr/bin/ping ## DEBIAN SERVICE=/usr/sbin/service tries=0 while [[ $tries -lt 3 ]] do if $PING -c 1 10.0.0.1 then echo \u0026quot;wg works\u0026quot; exit 0 fi echo \u0026quot;wg fail\u0026quot; tries=$((tries+1)) done echo \u0026quot;wg failed 3 times - restarting tunnel\u0026quot; ## DEBIAN $SERVICE wg-quick@wg0-client restart Please make sure the paths to certain binaries are congruent with your own system. If they are not, the script will fail. Some distributions put them in different places (e.g.: /bin/bash instead of /usr/bin/bash). If you are not sure where they are, you can do which binaryname that should report the full path to the binary.\n$ which bash /usr/bin/bash Make the script executable:\nchmod +x /usr/local/scripts/wg-watch.sh Once we have that done, we need to schedule it. I choose to schedule this every five minutes, but if you want to wait longer that is up to you.\nSchedule the script to run on a regular basis using cron. You can find out more about cron here: https://opensource.com/article/17/11/how-use-cron-linux\nWe\u0026rsquo;re going to use crontab to add this script to the list of jobs.\ncrontab -e Once the crontab editor is open, add this:\n*/5 * * * * /usr/local/scripts/wg-watch.sh Write and close the file. Crontab should confirm that it has been updated.\nYou should be set with a WireGuard VPN tunnel between a server and a client along with a script to bring the tunnel back up if it fails.\n"
}]